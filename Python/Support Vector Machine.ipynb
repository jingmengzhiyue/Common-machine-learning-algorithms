{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Introduction of Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "支持向量机（Support Vector Machine, SVM）是一类按监督学习（supervised learning）方式对数据进行二元分类的广义线性分类器（generalized linear classifier），其决策边界是对学习样本求解的最大边距超平面(maximum-margin hyperplane)。支持向量机(svm)的想法与前面介绍的感知机模型类似，找一个超平面将正负样本分开，但svm的想法要更深入了一步，它要求正负样本中离超平面最近的点的距离要尽可能的大，所以svm模型建模可以分为两个子问题： \n",
    "1. 将正负样本尽可能分开\n",
    "2. 让两类样本离超平面距离尽可能远"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM的目标函数可以表示为：\n",
    "$$\n",
    "f(x)=sign(w^Tx+b)\n",
    "$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于样本对$(x,y)$，只要$w^Tx+b$和$y$的符号相同，就意味着分对了，即$y\\cdot (w^Tx+b)>0$。解决了分对的问题后要解决怎样使得超平面离两类样本点尽可能远即\n",
    "$$\n",
    "\\max ~d\n",
    "$$\n",
    "当达到要求时，超平面距离正负样本的距离一定是一样的，点到超平面的距离表示为：\n",
    "$$\n",
    "d=\\frac{|w^Tx+b|}{||w||}\n",
    "$$  \n",
    "参数$w,b$可以等比例的变化，而不会影响到模型自身，所以$|w^Tx+b|=1$自然也可以满足，所以这时最近的点的距离可以表示为：\n",
    "\n",
    "$$\n",
    "d^*=\\frac{1}{||w||}\n",
    "$$\n",
    "同时第一个子问题的条件要调整为$y\\cdot(w^Tx+b)\\geq1$，而$\\max d^*$可以等价的表示为$\\min \\frac{1}{2}w^Tw$，所以svm模型的求解可以表述为如下优化问题：  \n",
    "\n",
    "$$\n",
    "\\max _{w,b} \\frac{1}{w^Tw} \\\\\n",
    "s.t.~~~~y_i(w^Tx_i+b)\\geq 1,i=1,2,...,N\n",
    "$$\n",
    "通常情况下我们习惯写成最小化问题，因此可以重新表示为：\n",
    "$$\n",
    "\\min_{w,b} \\frac{1}{2}w^Tw \\\\\n",
    "s.t.~~~~y_i(w^Tx_i+b)\\geq 1,i=1,2,...,N\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lagrangian Duality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于上面优化问题的求解往往转化为对其对偶问题的求解，首先，构造其拉格朗日函数：\n",
    "$$\n",
    "L(w,b,\\lambda)=\\frac{1}{2}w^Tw+\\sum_{i=1}^N \\lambda_i(1-y_i(w^Tx_i+b)),\\lambda=[\\lambda_1,\\lambda_2,...,\\lambda_N]\n",
    "$$  \n",
    "这时，原优化问题（设为$P$）就等价于：  \n",
    "\n",
    "$$\n",
    "\\min_{w,b}\\max_{\\lambda}L(w,b,\\lambda)\\\\\n",
    "s.t.~~~~\\lambda_i\\geq 0,i=1,2,...,N\n",
    "$$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Karush–Kuhn–Tucker conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先最优解一定满足原问题的约束条件：\n",
    "$$\n",
    "1-y_i({w^*}^Tx_i+b)\\leq0,i=1,2,...,N\n",
    "$$  \n",
    "其次还要满足对偶问题的约束条件：\n",
    "$$\n",
    "\\lambda_i^*\\geq0,i=1,2,...,N\\\\\n",
    "$$\n",
    "互补松弛条件：\n",
    "$$\n",
    "\\lambda_i^*(1-y_i({w^*}^Tx_i+b^*))=0,i=1,2,...,N\n",
    "$$  \n",
    "$L(w,b,\\lambda)$关于$w,b$的偏导为0，即：\n",
    "$$\n",
    "w=\\sum_{i=1}^N\\lambda_iy_ix_i\\\\\n",
    "0=\\sum_{i=1}^N\\lambda_iy_i\n",
    "$$\n",
    "将上述两式$w$和$b$带入对偶问题可得：\n",
    "$$\n",
    "\\max_{\\lambda} \\sum_{i=1}^N\\lambda_i-\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\lambda_i\\lambda_jy_iy_jx_i^Tx_j\\\\\n",
    "s.t.\\sum_{i=1}^N\\lambda_iy_i=0,\\\\\n",
    "\\lambda\\geq0,i=1,2,...,N\n",
    "$$  \n",
    "将最大化问题转化为最小化问题有：\n",
    "$$\n",
    "\\min_{\\alpha} \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_jx_i^Tx_j-\\sum_{i=1}^N\\alpha_i\\\\\n",
    "s.t.\\sum_{i=1}^N\\alpha_iy_i=0,\\\\\n",
    "\\alpha_i\\geq0,i=1,2,...,N\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
